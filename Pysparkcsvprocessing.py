import os
import glob
from pyspark.sql import SparkSession
from pyspark.sql.functions import count, col, regexp_replace, trim, to_date, month
from pyspark.sql.types import IntegerType, StringType, DateType
import pandas as pd
import time

# Start Timer
start_time = time.time()

#Set environment paths for Hadoop & Java
#https://github.com/cdarlint/winutils,
#download extract zip file and utilize hadoop-3.3.6 folder
os.environ["HADOOP_HOME"] = "D:/Python-Projects/DatapipelineAmazon/hadoop-3.3.6"
os.environ["HADOOP_BIN"] = "D:/Python-Projects/DatapipelineAmazon/hadoop-3.3.6/bin"
os.environ["JAVA_HOME"] = "C:/Program Files/Java/jdk1.8.0_321"
os.environ["PATH"] += os.pathsep + os.environ["HADOOP_BIN"]

# Initialize Spark Session
spark = SparkSession.builder.appName("AWSDataCleaning").config("spark.driver.memory", "2g").getOrCreate()

# Load CSV File
local_file_path = "./input_file/awsdata.csv"
df = spark.read.csv(local_file_path, header=True, inferSchema=True)
print("\n CSV File Schema:")
df.printSchema()
total_rows = df.count()
print(f"\n Total Rows in File: {total_rows}")
print("\n Now Showing First 5 Rows:")
df.show(5, truncate=False)

print("Data cleaning and transformation started")
# Find Duplicate review_id Values
duplicate_ids = df.groupBy("review_id").agg(count("*").alias("count")).filter("count > 1")
print("\n Duplicate review_id List:")
# Show first 10 duplicate review_id's
duplicate_ids.show(10, truncate=False)
# Remove Duplicates
df = df.dropDuplicates()
total_rows_after = df.count()
print(f"\n Total Rows (After Deduplication): {total_rows_after}")

# Handle Missing Values
df = df.fillna({
    "review_headline": "No Title",
    "review_body": "No Review",
    "product_category": "Unknown",
    "marketplace": "Unknown",
    "star_rating": 0  # Default rating to 0 if missing
})

# Convert Data Types
df = df.withColumn("star_rating", col("star_rating").cast(IntegerType())) \
       .withColumn("review_date", to_date(col("review_date"), "yyyy-MM-dd"))

# Standardize Text Fields
df = df.withColumn("review_headline", trim(regexp_replace(col("review_headline"), r"[^a-zA-Z0-9\s]", ""))) \
       .withColumn("review_body", trim(regexp_replace(col("review_body"), r"[^a-zA-Z0-9\s]", ""))) \
       .withColumn("product_category", trim(regexp_replace(col("product_category"), r"[^a-zA-Z0-9\s]", ""))) \
       .withColumn("marketplace", trim(regexp_replace(col("marketplace"), r"[^a-zA-Z0-9\s]", "")))

# Create New Feature: Extract Review Month
df = df.withColumn("review_month", month(col("review_date")))

# Save Cleaned & Transformed Data
output_dir = "./processed_data/"
df.write.csv(output_dir, mode="overwrite", header=True)
# generally in output directory we will see below things
# part-00001-xxxxx.csv: Actual data file partition indicators
# _SUCCESS: Indicates successful execution of the job.
# .crc files: Checksum files generated by Hadoop for integrity verification.
# Stop Spark Session
spark.stop()

# merge all csv partition
csv_files = sorted(glob.glob(os.path.join(output_dir, "part-*.csv")))
final_output_dir = os.path.abspath("./output_file")
final_output_path = os.path.join(final_output_dir, "final_cleaned_awsdata.csv")
if csv_files:
    print(f"Merging {len(csv_files)} partitioned CSV files...")
    # Read all partitioned CSVs and append them
    merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)
    # Save the combined data to final CSV
    merged_df.to_csv(final_output_path, index=False)
    print(f"Final cleaned CSV generated: {final_output_path}")
    print(f"All partitioned CSVs are preserved in {output_dir}")
else:
    print("No CSV files found in processed_data folder!")

print("Data cleaning and transformation completed")

end_time = time.time()
total_seconds = end_time - start_time
formatted_time = time.strftime("%H:%M:%S", time.gmtime(total_seconds))
print(f" Total Execution Time: {formatted_time}")

